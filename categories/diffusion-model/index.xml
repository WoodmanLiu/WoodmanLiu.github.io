<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Diffusion Model on A minimal Hugo website</title>
    <link>http://localhost:1313/categories/diffusion-model/</link>
    <description>Recent content in Diffusion Model on A minimal Hugo website</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 16 Jan 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/diffusion-model/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DDPM 数学原理推导</title>
      <link>http://localhost:1313/blog/ddpm-%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%E6%8E%A8%E5%AF%BC/</link>
      <pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/ddpm-%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%E6%8E%A8%E5%AF%BC/</guid>
      <description>&lt;p&gt;首先明确生成任务的目标：训练一个神经网络模型，让它把一个从已知概率分布(如标准正态分布)中采样的数据，映射到与真实数据分布相似的结果。换句话说，模型通过学习将简单的随机输入转化为符合真实数据特性的样本，从而实现数据生成。DDPM (Denoising Diffusion Probabilistic Models) 包含两个主要过程：&lt;strong&gt;前向过程&lt;/strong&gt;和&lt;strong&gt;反向过程&lt;/strong&gt;。在前向过程中逐步向原始图像添加噪声，将数据从真实分布转化为简单的高斯分布；而反向过程从高斯噪声开始，逐步去噪，最终恢复数据分布。&lt;/p&gt;&#xA;&lt;h1 id=&#34;1-前向过程&#34;&gt;1. 前向过程&lt;/h1&gt;&#xA;&lt;p&gt;给定一个从真实数据分布中采样的图像 &lt;code&gt;$\mathbf{x}_0\sim q(\mathbf{x})$&lt;/code&gt;，我们逐步向其添加 &lt;code&gt;$T$&lt;/code&gt; 次高斯噪声，得到一系列带噪图像 &lt;code&gt;$\mathbf{x}_1,...,\mathbf{x}_T$&lt;/code&gt;，这个过程是一个马尔可夫链：&#xA;&lt;code&gt;$$\begin{equation}q(\mathbf{x}_t|\mathbf{x}_{t-1})=\mathcal{N}(\mathbf{x}_t;\sqrt{1-\beta_t}\mathbf{x}_{t-1}, \beta_t\mathbf{I})\label{eq1}\end{equation}$$&lt;/code&gt;&#xA;其中 &lt;code&gt;$\beta_t$&lt;/code&gt; 是一个控制噪声大小的参数，范围在 &lt;code&gt;$(0,1)$&lt;/code&gt; 且接近于零，该参数随着前向扩散过程的逐步进行而增大。定义 &lt;code&gt;$\alpha_t=1-\beta_t$&lt;/code&gt;，通过重参数化技巧，可以将上式改写为：&#xA;&lt;code&gt;$$\begin{equation}\mathbf{x}_t=\sqrt\alpha_t \mathbf{x}_{t-1}+\sqrt{1-\alpha_t}\boldsymbol{\epsilon}_{t-1}\label{eq2}\end{equation}$$&lt;/code&gt;&#xA;其中，&lt;code&gt;$\boldsymbol{\epsilon}_{t-1}\sim\mathcal{N}(\mathbf{0},\mathbf{I})$&lt;/code&gt;。这个公式可以这样形象地理解：把图像变淡一点，并在这个基础上撒一点噪声。定义 &lt;code&gt;$\bar{\alpha}_t=\prod_{s=1}^t\alpha_s$&lt;/code&gt;，并递归地进行推导：&#xA;&lt;code&gt;$$\begin{equation}\begin{split}\mathbf{x}_t&amp;amp;=\sqrt\alpha_t (\sqrt\alpha_{t-1} \mathbf{x}_{t-2}+\sqrt{1-\alpha_{t-1}}\boldsymbol{\epsilon}_{t-2})+\sqrt{1-\alpha_t}\boldsymbol{\epsilon}_{t-1}\\&amp;amp;=\sqrt{\alpha_t \alpha_{t-1}}\mathbf{x}_{t-2}+\sqrt{\alpha_t-\alpha_t\alpha_{t-1}}\boldsymbol{\epsilon}_{t-2}+\sqrt{1-\alpha_t\boldsymbol{\epsilon}_{t-1}}\\&amp;amp;=\sqrt{\alpha_t \alpha_{t-1}}\mathbf{x}_{t-2}+(1-\alpha_t\alpha_{t-1})\boldsymbol{\bar\epsilon}_{t-2}\\&amp;amp;=...\\&amp;amp;=\sqrt{\bar\alpha_t}\mathbf{x}_0+(1-\bar\alpha_t)\boldsymbol{\epsilon}\end{split}\label{eq3}\end{equation}$$&lt;/code&gt;&#xA;其中 &lt;code&gt;$\boldsymbol{\bar\epsilon}_{t-2}$&lt;/code&gt; 和 &lt;code&gt;$\boldsymbol{\epsilon}$&lt;/code&gt; 是标准正态分布叠加在一起的结果，它们仍然服从标准正态分布。由上式可以看出，当 &lt;code&gt;$t$&lt;/code&gt; 特别大时，&lt;code&gt;$\bar\alpha_t$&lt;/code&gt; 接近于零，&lt;code&gt;$\mathbf{x}_t$&lt;/code&gt; 就变成了零均值的高斯噪声。我们可以由 &lt;code&gt;$\mathbf{x}_0$&lt;/code&gt; 一步推导出 &lt;code&gt;$\mathbf{x}_t$&lt;/code&gt;：&#xA;&lt;code&gt;$$\begin{equation}q(\mathbf{x}_t|\mathbf{x}_0)=\mathcal{N}(\mathbf{x}_t;\sqrt{\bar\alpha_t}\mathbf{x}_0, (1-\bar\alpha_t)\mathbf{I})\label{eq4}\end{equation}$$&lt;/code&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;2-反向过程&#34;&gt;2. 反向过程&lt;/h1&gt;&#xA;&lt;p&gt;反向过程与前向过程相反，是一个去噪的过程。如果我们知道反向过程每一步的分布 &lt;code&gt;$q(\mathbf{x}_{t-1}|\mathbf{x}_t)$&lt;/code&gt;，那么就可以从一个随机的高斯噪声开始，逐步去噪，最终得到干净的数据。分布 &lt;code&gt;$q(\mathbf{x}_{t-1}|\mathbf{x}_t)$&lt;/code&gt; 很难直接求得，但加上条件 &lt;code&gt;$\mathbf{x_0}$&lt;/code&gt; 时，我们把反向过程的每一小步也近似为高斯分布，概率 &lt;code&gt;$q(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)$&lt;/code&gt; 是可处理的：&#xA;&lt;code&gt;$$\begin{equation}q(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)=\mathcal{N}(\mathbf{x}_{t-1};\tilde{\mu}(\mathbf{x}_t,\mathbf{x}_0),\tilde{\beta}_t\mathbf{I})\label{eq5}\end{equation}$$&lt;/code&gt;&#xA;根据贝叶斯公式：&#xA;&lt;code&gt;$$\begin{equation}q(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)=\frac{q(\mathbf{x}_t|\mathbf{x}_{t-1},\mathbf{x}_0)q(\mathbf{x}_{t-1}|\mathbf{x}_0)}{q(\mathbf{x}_t|\mathbf{x}_0)}\label{eq6}\end{equation}$$&lt;/code&gt;&#xA;因为反向过程的每一小步也具有马尔可夫性质，所以有 &lt;code&gt;$q(\mathbf{x}_t|\mathbf{x}_{t-1},\mathbf{x}_0)=q(\mathbf{x}_t|\mathbf{x}_{t-1})$&lt;/code&gt;，因此式(&lt;code&gt;$\ref{eq6}$&lt;/code&gt;)等号右边的三个概率分布可以根据式(&lt;code&gt;$\ref{eq1}$&lt;/code&gt;)和式(&lt;code&gt;$\ref{eq4}$&lt;/code&gt;)写出具体的表达式。将其带入式(&lt;code&gt;$\ref{eq6}$&lt;/code&gt;)之后，按照式(&lt;code&gt;$\ref{eq5}$&lt;/code&gt;)高斯分布的概率密度函数的形式，可以得到 &lt;code&gt;$q(\mathbf{x}_{t-1}|\mathbf{x}_t,\mathbf{x}_0)$&lt;/code&gt; 的均值和方差：&#xA;&lt;code&gt;$$\begin{equation}\tilde{\mu}_t=\frac{1}{\sqrt{\alpha_t}}(\mathbf{x}_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_t)\label{eq7}\end{equation}$$&lt;/code&gt;&#xA;&lt;code&gt;$$\begin{equation}\tilde{\beta}_t=\frac{1-\overline{\alpha}_{t-1}}{1-\overline{\alpha}_t}\beta_t\label{eq8}\end{equation}$$&lt;/code&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
